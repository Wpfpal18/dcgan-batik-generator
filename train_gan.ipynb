{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "nyY-oRlMdHMh"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import tensorflow as tf\n",
        "import time\n",
        "\n",
        "DATASET_ROOT = \"projects\\data\"\n",
        "IMG_SIZE = 64\n",
        "LATENT_DIM = 128\n",
        "BATCH_SIZE = 64\n",
        "EPOCHS = 3000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "MvPiXtvEX09K"
      },
      "outputs": [],
      "source": [
        "tf.random.set_seed(42)\n",
        "np.random.seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "id": "FIAuX92HhB3G",
        "outputId": "20ffd938-44e5-4106-bfc9-7852f716704c"
      },
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "[WinError 3] The system cannot find the path specified: 'projects\\\\data'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[9], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDATASET_ROOT\u001b[49m\u001b[43m)\u001b[49m)\n",
            "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] The system cannot find the path specified: 'projects\\\\data'"
          ]
        }
      ],
      "source": [
        "print(os.listdir(DATASET_ROOT))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CVtxoffodqxW"
      },
      "outputs": [],
      "source": [
        "def load_images_multiclass(root):\n",
        "    images = []\n",
        "\n",
        "    for folder in os.listdir(root):\n",
        "        folder_path = os.path.join(root, folder)\n",
        "        if not os.path.isdir(folder_path):\n",
        "            continue\n",
        "\n",
        "        for file in os.listdir(folder_path):\n",
        "            if not file.lower().endswith(\".jpg\"):\n",
        "                continue\n",
        "\n",
        "            img_path = os.path.join(folder_path, file)\n",
        "\n",
        "            try:\n",
        "                img = Image.open(img_path).convert(\"RGB\")\n",
        "            except:\n",
        "                continue\n",
        "\n",
        "            img = img.resize((IMG_SIZE, IMG_SIZE))\n",
        "            img = np.array(img)\n",
        "            img = (img - 127.5) / 127.5\n",
        "            images.append(img)\n",
        "\n",
        "    return np.array(images)\n",
        "\n",
        "data = load_images_multiclass(DATASET_ROOT)\n",
        "print(\"Total images:\", data.shape[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vwWRx1PYkgui"
      },
      "outputs": [],
      "source": [
        "dataset = tf.data.Dataset.from_tensor_slices(data).shuffle(2000).batch(BATCH_SIZE).prefetch(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GQkZfl8Xd71R"
      },
      "outputs": [],
      "source": [
        "def build_generator():\n",
        "  m = tf.keras.Sequential()\n",
        "  m.add(tf.keras.layers.Dense(8 * 8 * 128, input_dim=LATENT_DIM))\n",
        "  m.add(tf.keras.layers.Reshape((8, 8, 128)))\n",
        "  m.add(tf.keras.layers.Conv2DTranspose(64, 4, strides=2, padding='same'))\n",
        "  m.add(tf.keras.layers.BatchNormalization())\n",
        "  m.add(tf.keras.layers.ReLU())\n",
        "  m.add(tf.keras.layers.Conv2DTranspose(32, 4, strides=2, padding='same'))\n",
        "  m.add(tf.keras.layers.BatchNormalization())\n",
        "  m.add(tf.keras.layers.ReLU())\n",
        "  m.add(tf.keras.layers.Conv2DTranspose(3, 4, strides=2, padding='same', activation='tanh'))\n",
        "  return m\n",
        "\n",
        "def build_descriminator():\n",
        "  m = tf.keras.Sequential()\n",
        "  m.add(tf.keras.layers.Conv2D(32, 4, strides=2, padding=\"same\", input_shape=(64, 64, 3)))\n",
        "  m.add(tf.keras.layers.LeakyReLU(0.2))\n",
        "  m.add(tf.keras.layers.Conv2D(64, 4, strides=2, padding=\"same\"))\n",
        "  m.add(tf.keras.layers.LeakyReLU(0.2))\n",
        "  m.add(tf.keras.layers.Flatten())\n",
        "  m.add(tf.keras.layers.Dense(1))\n",
        "  return m\n",
        "\n",
        "generator = build_generator()\n",
        "discriminator = build_descriminator()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UAV43vDiiJK-"
      },
      "outputs": [],
      "source": [
        "loss_fn = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "opt_g = tf.keras.optimizers.Adam(0.0002, 0.5)\n",
        "opt_d = tf.keras.optimizers.Adam(0.0001, 0.5)\n",
        "\n",
        "@tf.function\n",
        "def train_step(real_imgs):\n",
        "  bs =  tf.shape(real_imgs)[0]\n",
        "  noise = tf.random.normal([bs, LATENT_DIM])\n",
        "\n",
        "  with tf.GradientTape() as d_tape:\n",
        "    fake_imgs = generator(noise, training=True)\n",
        "    real_logits = discriminator(real_imgs, training=True)\n",
        "    fake_logits = discriminator(fake_imgs, training=True)\n",
        "    d_loss_real = loss_fn(tf.ones_like(real_logits) * 0.9, real_logits)\n",
        "    d_loss_fake = loss_fn(tf.zeros_like(fake_logits), fake_logits)\n",
        "    d_loss = d_loss_real + d_loss_fake\n",
        "\n",
        "  grads_d = d_tape.gradient(d_loss, discriminator.trainable_variables)\n",
        "  opt_d.apply_gradients(zip(grads_d, discriminator.trainable_variables))\n",
        "\n",
        "  for _ in range(2):\n",
        "    noise = tf.random.normal([bs, LATENT_DIM])\n",
        "    with tf.GradientTape() as tape:\n",
        "      fake_imgs = generator(noise, training=True)\n",
        "      fake_logits = discriminator(fake_imgs, training=True)\n",
        "      g_loss =  loss_fn(tf.ones_like(fake_logits), fake_logits)\n",
        "    grads = tape.gradient(g_loss, generator.trainable_variables)\n",
        "    opt_g.apply_gradients(zip(grads, generator.trainable_variables))\n",
        "\n",
        "  return d_loss, g_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gmt-kO7cjMMS"
      },
      "outputs": [],
      "source": [
        "if not os.path.exists(\"output\"):\n",
        "  os.makedirs(\"output\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-ZpQ9F1wXCwW"
      },
      "outputs": [],
      "source": [
        "def save_grid(generator, epoch, n=4):\n",
        "  noise = tf.random.normal([n*n, LATENT_DIM])\n",
        "  imgs = generator(noise, training=False)\n",
        "  imgs = (imgs * 127.5 + 127.5).numpy().astype(np.uint8)\n",
        "\n",
        "  grid = Image.new(\"RGB\", (IMG_SIZE*n, IMG_SIZE*n))\n",
        "  idx = 0\n",
        "  for i in range(n):\n",
        "    for j in range(n):\n",
        "      grid.paste(Image.fromarray(imgs[idx]), (j*IMG_SIZE, i*IMG_SIZE))\n",
        "      idx += 1\n",
        "\n",
        "  grid.save(\"output/grid_\" + str(epoch) + \".png\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p41LIMYKjQyR"
      },
      "outputs": [],
      "source": [
        "d_losses = []\n",
        "g_losses = []\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "  for batch in dataset:\n",
        "    d_loss, g_loss = train_step(batch)\n",
        "\n",
        "  if epoch % 100 == 0:\n",
        "    print(\"epoch\", epoch, \"D\", float(d_loss), \"G\", float(g_loss))\n",
        "    noise = tf.random.normal([1, LATENT_DIM])\n",
        "    img = generator(noise, training=False)[0].numpy()\n",
        "    img = (img * 127.5 + 127.5).astype(np.uint8)\n",
        "    Image.fromarray(img).save(\"output/sample_\" + str(epoch) + \".png\")\n",
        "\n",
        "  if epoch % 1000 == 0:\n",
        "    generator.save(\"output/generator_\" + str(epoch) + \".keras\")\n",
        "    discriminator.save(\"output/discriminator_\" + str(epoch) + \".keras\")\n",
        "\n",
        "  if epoch % 500 == 0:\n",
        "    save_grid(generator, epoch)\n",
        "\n",
        "  d_losses.append(float(d_loss))\n",
        "  g_losses.append(float(g_loss))\n",
        "\n",
        "  print(\"Training time (minutes):\", (time.time() - start_time) / 60)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dtdj-3xuW-j3"
      },
      "outputs": [],
      "source": [
        "generator.save(\"output/generator_final.keras\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tdVRKRCyX5Xj"
      },
      "outputs": [],
      "source": [
        "img_check = (data[0] * 127.5 + 127.5).astype(np.uint8)\n",
        "Image.fromarray(img_check)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "dl",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
